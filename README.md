# 본 스크립트들은 robots.txt를 준수합니다. 

> robots.txt란?
단순하게 스크래핑을 무엇을하지 말아야하는지 규정해주는 가이드라인이라 할수있습니다.  
scraping은 주로 자기가 원하는 웹사이트의 핵심 내용들을 주기적으로 뽑아야하는 등과 같은 행위를 말합니다.    
참고로 robots.txt에 나오는 것들과 저작권이 있는 데이터들은  스크래핑하면 안됩니다.  
당연하게도 상업적으로 데이터를 활용할시에 사용자가 책임져야할 사항이기에 주의해야합니다.    

# 환경
24시간 끊임없이 스크래핑을 해야되면 colab을 추천합니다.
설정자체도 어렵지않고 자원도 거의 부족하다 느끼지 못할만큼 넘치고
가격도 ec2와 vpc에 비해 가격도 저렴하며 신경써야할것이 적기 때문입니다.  
스크래핑한 데이터를 기반하여 연구도 하기 쉽습니다. colab은 ai 머신러닝과 딥러닝을 하기위해 만든 사이트기 때문입니다.

